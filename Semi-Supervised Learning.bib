Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Li2019,
abstract = {Community detection was a hot topic on network analysis, where the main aim is to perform unsupervised learning or clustering in networks. Recently, semi-supervised learning has received increasing attention among researchers. In this paper, we propose a new algorithm, called weighted inverse Laplacian (WIL), for predicting labels in partially labeled networks. The idea comes from the first hitting time in random walk, and it also has nice explanations both in information propagation and the regularization framework. We propose a partially labeled degree-corrected block model (pDCBM) to describe the generation of partially labeled networks. We show that WIL ensures the misclassification rate is of order {\$}O(\backslashfrac{\{}1{\}}{\{}d{\}}){\$} for the pDCBM with average degree {\$}d=\backslashOmega(\backslashlog n),{\$} and that it can handle situations with greater unbalanced than traditional Laplacian methods. WIL outperforms other state-of-the-art methods in most of our simulations and real datasets, especially in unbalanced networks and heterogeneous networks.},
archivePrefix = {arXiv},
arxivId = {1901.01696},
author = {Li, Ting and Ying, Ningchen and Yu, Xianshi and Jing, Bin-Yi},
eprint = {1901.01696},
file = {:E$\backslash$:/360yunpan/Mendeley/semi-supervised learning in unbalanced and heterogeneous networks.pdf:pdf},
pages = {1--31},
title = {{Semi-supervised learning in unbalanced and heterogeneous networks}},
url = {http://arxiv.org/abs/1901.01696},
year = {2019}
}
@article{Dumstorff2007,
author = {Dumstorff, Peter},
doi = {10.1002/nag},
file = {:E$\backslash$:/360yunpan/Mendeley/semi-supervised self-training approaches for imbalanced splice site datasets.pdf:pdf},
issn = {03639061},
journal = {International Journal for Numerical and Analytical Methods in Geomechanics},
number = {November 2006},
pages = {239--259},
title = {{Semi-Supervised Self-training Approaches for imbalanced Splice Site Datasets}},
year = {2007}
}
@article{Su2012,
abstract = {Standard supervised approach to sentiment classification requires a large amount of manually labeled data which is costly and time-consuming to obtain. To tackle this problem, we propose a novel semi-supervised learning method based on multi-view learning. The main idea of our approach is generate multiple views by exploiting both feature partition and language translation strategies and then standard co-training algorithm is applied to perform multi-view learning for semi-supervised sentiment classification. Empirical study across four domains demonstrates the effectiveness of our approach. {\textcopyright} 2012 IEEE.},
author = {Su, Yan and Li, Shoushan and Ju, Shengfeng and Zhou, Guodong and Li, Xiaojun},
doi = {10.1109/IALP.2012.53},
file = {:E$\backslash$:/360yunpan/Mendeley/semi-supervised learning for imbalanced sentiment classification.pdf:pdf},
journal = {Proceedings - 2012 International Conference on Asian Language Processing, IALP 2012},
keywords = {Cross-language,Semi-supervised,Sentiment classification},
pages = {13--16},
title = {{Multi-view learning for semi-supervised sentiment classification}},
year = {2012}
}
@article{Tamposis2019,
abstract = {Motivation: Hidden Markov Models (HMMs) are probabilistic models widely used in applications in computational sequence analysis. HMMs are basically unsupervised models. However, in the most important applications, they are trained in a supervised manner. Training examples accompanied by labels corresponding to different classes are given as input and the set of parameters that maximize the joint probability of sequences and labels is estimated. A main problem with this approach is that, in the majority of the cases, labels are hard to find and thus the amount of training data is limited. On the other hand, there are plenty of unclassified (unlabeled) sequences deposited in the public databases that could potentially contribute to the training procedure. This approach is called semi-supervised learning and could be very helpful in many applications. Results: We propose here, a method for semi-supervised learning of HMMs that can incorporate labeled, unlabeled and partially labeled data in a straightforward manner. The algorithm is based on a variant of the Expectation-Maximization (EM) algorithm, where the missing labels of the unlabeled or partially labeled data are considered as the missing data. We apply the algorithm to several biological problems, namely, for the prediction of transmembrane protein topology for alpha-helical and beta-barrel membrane proteins and for the prediction of archaeal signal peptides. The results are very promising, since the algorithms presented here can significantly improve the prediction performance of even the top-scoring classifiers.},
author = {Tamposis, Ioannis A. and Tsirigos, Konstantinos D. and Theodoropoulou, Margarita C. and Kontou, Panagiota I. and Bagos, Pantelis G.},
doi = {10.1093/bioinformatics/bty910},
file = {:E$\backslash$:/360yunpan/Mendeley/Semi-supervised learning of Hidden Markov Models for biological sequence analysis.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {13},
pages = {2208--2215},
title = {{Semi-supervised learning of hidden markov models for biological sequence analysis}},
volume = {35},
year = {2019}
}
