Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Zhu2017,
abstract = {Recently, pedestrian attributes like gender, age, clothing etc., have been used as soft biometric traits for recognizing people. Unlike existing methods that assume the independence of attributes during their prediction, we propose a multi-label convolutional neural network (MLCNN) to predict multiple attributes together in a unified framework. Firstly, a pedestrian image is roughly divided into multiple overlapping body parts, which are simultaneously integrated in the multi-label convolutional neural network. Secondly, these parts are filtered independently and aggregated in the cost layer. The cost function is a combination of multiple binary attribute classification cost functions. Experiments show that the proposed method significantly outperforms the SVM based method on the PETA database.},
author = {Zhu, Jianqing and Liao, Shengcai and Lei, Zhen and Li, Stan Z.},
doi = {10.1016/j.imavis.2016.07.004},
file = {:E$\backslash$:/360yunpan/Mendeley/Multi-label convolutional neural network based pedestrian attribute classification.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Convolutional neural network,Multi-label classification,Pedestrian attribute classification},
pages = {224--229},
publisher = {Elsevier B.V.},
title = {{Multi-label convolutional neural network based pedestrian attribute classification}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.07.004},
volume = {58},
year = {2017}
}
@article{Gao2013,
abstract = {Multi-label learning has attracted much attention during the past few years. Many multi-label approaches have been developed, mostly working with surrogate loss functions because multi-label loss functions are usually difficult to optimize directly owing to their non-convexity and discontinuity. These approaches are effective empirically, however, little effort has been devoted to the understanding of their consistency, i.e., the convergence of the risk of learned functions to the Bayes risk. In this paper, we present a theoretical analysis on this important issue. We first prove a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Then, we study the consistency of two well-known multi-label loss functions, i.e., ranking loss and hamming loss. For ranking loss, our results disclose that, surprisingly, none of convex surrogate loss is consistent; we present the partial ranking loss, with which some surrogate losses are proven to be consistent. We also discuss on the consistency of univariate surrogate losses. For hamming loss, we show that two multi-label learning methods, i.e., one-vs-all and pairwise comparison, which can be regarded as direct extensions from multi-class learning, are inconsistent in general cases yet consistent under the dominating setting, and similar results also hold for some recent multi-label approaches that are variations of one-vs-all. In addition, we discuss on the consistency of learning approaches that address multi-label learning by decomposing into a set of binary classification problems.},
author = {Gao, Wei and Zhou, Zhi-Hua},
doi = {http://dx.doi.org/10.1016/j.artint.2013.03.001},
file = {:E$\backslash$:/360yunpan/Mendeley/1-s2.0-S0004370213000313-main.pdf:pdf},
isbn = {0004-3702},
journal = {Artificial Intelligence},
keywords = {Bayes consistency,Hamming loss,Machine learning,Multi-label learning,Ranking loss,Surrogate loss},
number = {0},
pages = {22--44},
title = {{On the consistency of multi-label learning}},
url = {http://www.sciencedirect.com/science/article/pii/S0004370213000313},
volume = {199â€“200},
year = {2013}
}
@article{Qiu2017c,
abstract = {Predicting phosphorylation protein is a challenging problem, particularly when query proteins have multi-label features meaning that they may be phosphorylated at two or more different type amino acids. In fact, human protein usually be phosphorylated at serine, threonine and tyrosine. By introducing the "multi-label learning" approach, a novel predictor has been developed that can be used to deal with the systems containing both single- and multi-label phosphorylation protein. Here we proposed a predictor called Multi-iPPseEvo by (1) incorporating the protein sequence evolutionary information into the general pseudo amino acid composition (PseAAC) via the grey system theory, (2) balancing out the skewed training datasets by the asymmetric bootstrap approach, and (3) constructing an ensemble predictor by fusing an array of individual random forest classifiers thru a voting system. Rigorous cross-validations via a set of multi-label metrics indicate that the multi-label phosphorylation predictor is very promising and encouraging. The current approach represents a new strategy to deal with the multi-label biological problems, and the software is freely available for academic use at http://www.jci-bioinfo.cn/Multi-iPPseEvo.},
annote = {Qiu, Wang-Ren
Zheng, Quan-Shu
Sun, Bi-Qian
Xiao, Xuan
eng
Germany
2016/09/30 06:00
Mol Inform. 2017 Mar;36(3). doi: 10.1002/minf.201600085. Epub 2016 Sep 29.},
author = {Qiu, W R and Zheng, Q S and Sun, B Q and Xiao, X},
doi = {10.1002/minf.201600085},
file = {:E$\backslash$:/360yunpan/Mendeley/Multi-iPPseEvo A multi-label classifier for identifying human phosphorylated proteins by incorporating evolutionary information into Cho.pdf:pdf},
isbn = {1868-1751 (Electronic)
1868-1743 (Linking)},
journal = {Molecular Informatics},
number = {3},
pmid = {27681207},
title = {{Multi-iPPseEvo: A Multi-label Classifier for Identifying Human Phosphorylated Proteins by Incorporating Evolutionary Information into Chou's General PseAAC via Grey System Theory}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27681207},
volume = {36},
year = {2017}
}
@article{Sun2017,
abstract = {Multi-label learning is concerned with learning from data examples that are represented by a single feature vector while associated with multiple labels simultaneously. Existing multi-label learning approaches mainly focus on exploiting label correlations to facilitate the learning process. However, an intrinsic characteristic of multi-label learning, i.e., class-imbalance has not been well studied. In this paper, we propose a two-stage multi-label hypernetwork (TSMLHN) to exploit label correlations and to make use of them to address the class-imbalance problem in multi-label learning. In TSMLHN, labels of a multi-label data set are divided into two groups, i.e., imbalanced labels and common labels based on their imbalance ratios. In the first stage of TSMLHN, we train a multi-label hypernetwork (MLHN) which generates basic predictions for all labels. In the second stage of TSMLHN, we train TSMLHN based on the predictions obtained from MLHN and utilize the correlations between common labels and imbalanced labels to improve the learning performance of imbalanced labels. Our proposed TSMLHN is conceptually simple, yet it is effective in addressing the class-imbalance problem in multi-label learning. Empirical studies on a broad range of multi-label data sets demonstrate that TSMLHN achieves competitive performance against state-of-the-art multi-label learning algorithms.},
author = {Sun, Kai Wei and Lee, Chong Ho},
doi = {10.1016/j.neucom.2017.05.049},
file = {:E$\backslash$:/360yunpan/Mendeley/Addressing class-imbalance in multi-label learning via two-stage multi-label hypernetwork.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Hypernetwork,Imbalanced learning,Multi-label learning},
month = {nov},
pages = {375--389},
title = {{Addressing class-imbalance in multi-label learning via two-stage multi-label hypernetwork}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231217308998},
volume = {266},
year = {2017}
}
@article{Dumstorff2007,
author = {Dumstorff, Peter},
doi = {10.1002/nag},
file = {:E$\backslash$:/360yunpan/Mendeley/semi-supervised self-training approaches for imbalanced splice site datasets.pdf:pdf},
issn = {03639061},
journal = {International Journal for Numerical and Analytical Methods in Geomechanics},
number = {November 2006},
pages = {239--259},
title = {{Semi-Supervised Self-training Approaches for imbalanced Splice Site Datasets}},
year = {2007}
}
@article{Kumar2017b,
author = {Kumar, Aviral and Ramakrishnan, Ganesh},
file = {:E$\backslash$:/360yunpan/Mendeley/Deep Learning Approaches to the Multi-Instance Multi-Label Learning Problem.pdf:pdf},
journal = {Department of computer science and engineering},
number = {Miml},
publisher = {Indian Institute of Technology Bombay},
title = {{Deep Learning Approaches to the Multi-Instance Multi-Label ( MIML ) Learning Problem}},
year = {2017}
}
@article{Dong2018,
abstract = {Model learning from class imbalanced training data is a long-standing and significant challenge for machine learning. In particular, existing deep learning methods consider mostly either class balanced data or moderately imbalanced data in model training, and ignore the challenge of learning from significantly imbalanced training data. To address this problem, we formulate a class imbalanced deep learning model based on batch-wise incremental minority (sparsely sampled) class rectification by hard sample mining in majority (frequently sampled) classes during model training. This model is designed to minimise the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes in an iterative batch-wise learning process. To that end, we introduce a Class Rectification Loss (CRL) function that can be deployed readily in deep network architectures. Extensive experimental evaluations are conducted on three imbalanced person attribute benchmark datasets (CelebA, X-Domain, DeepFashion) and one balanced object category benchmark dataset (CIFAR-100). These experimental results demonstrate the performance advantages and model scalability of the proposed batch-wise incremental minority class rectification model over the existing state-of-the-art models for addressing the problem of imbalanced data learning.},
annote = {Dong, Qi
Gong, Shaogang
Zhu, Xiatian
eng
2018/07/12 06:00
IEEE Trans Pattern Anal Mach Intell. 2018 May 3. doi: 10.1109/TPAMI.2018.2832629.},
author = {Dong, Q and Gong, S and Zhu, X},
doi = {10.1109/TPAMI.2018.2832629},
file = {:E$\backslash$:/360yunpan/Mendeley/Imbalanced Deep Learning by Minority Class Incremental Rectification.pdf:pdf},
isbn = {1939-3539 (Electronic)
0098-5589 (Linking)},
journal = {IEEE Trans Pattern Anal Mach Intell},
pmid = {29993438},
title = {{Imbalanced Deep Learning by Minority Class Incremental Rectification}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29993438 https://ieeexplore.ieee.org/document/8353718/},
year = {2018}
}
@article{Sasaki-Saito2017,
author = {Sasaki-Saito, Natsuko and Sawada, Yu and Ohmori, Shun and Omoto, Daisuke and Haruyama, Sanehito and Yoshioka, Manabu and Nishio, Daisuke and Nakamura, Motonobu},
doi = {10.1684/ejd.2017.3052},
file = {:E$\backslash$:/360yunpan/Mendeley/Learning from Labeled and Unlabeled Data with Label Propagation.pdf:pdf},
issn = {19524013},
journal = {European Journal of Dermatology},
number = {4},
pages = {442--443},
pmid = {28508753},
title = {{Endometriosis in the setting of Muckle-Wells syndrome treated with an IL-1$\beta$ antagonist}},
volume = {27},
year = {2017}
}
@misc{Chu2018,
author = {Chu, Hong-Min and Yeh, Chih-Kuan and Wang, Yu-Chiang Frank},
booktitle = {The European Conference on Computer Vision},
chapter = {400-415},
title = {{Deep Generative Models for Weakly-Supervised Multi-Label Classification}},
year = {2018}
}
@article{Li2019,
abstract = {Community detection was a hot topic on network analysis, where the main aim is to perform unsupervised learning or clustering in networks. Recently, semi-supervised learning has received increasing attention among researchers. In this paper, we propose a new algorithm, called weighted inverse Laplacian (WIL), for predicting labels in partially labeled networks. The idea comes from the first hitting time in random walk, and it also has nice explanations both in information propagation and the regularization framework. We propose a partially labeled degree-corrected block model (pDCBM) to describe the generation of partially labeled networks. We show that WIL ensures the misclassification rate is of order {\$}O(\backslashfrac{\{}1{\}}{\{}d{\}}){\$} for the pDCBM with average degree {\$}d=\backslashOmega(\backslashlog n),{\$} and that it can handle situations with greater unbalanced than traditional Laplacian methods. WIL outperforms other state-of-the-art methods in most of our simulations and real datasets, especially in unbalanced networks and heterogeneous networks.},
archivePrefix = {arXiv},
arxivId = {1901.01696},
author = {Li, Ting and Ying, Ningchen and Yu, Xianshi and Jing, Bin-Yi},
eprint = {1901.01696},
file = {:E$\backslash$:/360yunpan/Mendeley/semi-supervised learning in unbalanced and heterogeneous networks.pdf:pdf},
pages = {1--31},
title = {{Semi-supervised learning in unbalanced and heterogeneous networks}},
url = {http://arxiv.org/abs/1901.01696},
year = {2019}
}
@misc{Wehrmann2018,
abstract = {One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.},
address = {Proceedings of Machine Learning Research},
author = {Wehrmann, Jonatas and Cerri, Ricardo and Barros, Rodrigo},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Jennifer, Dy and Andreas, Krause},
file = {:E$\backslash$:/360yunpan/Mendeley/Hierarchical Multi-Label Classification Networks.pdf:pdf},
keywords = {pmlr-v80-wehrmann18a},
mendeley-tags = {pmlr-v80-wehrmann18a},
pages = {5075--5084},
publisher = {PMLR {\%}J Proceedings of Machine Learning Research},
title = {{Hierarchical Multi-Label Classification Networks}},
url = {http://proceedings.mlr.press},
volume = {80},
year = {2018}
}
@article{Su2012a,
abstract = {Standard supervised approach to sentiment classification requires a large amount of manually labeled data which is costly and time-consuming to obtain. To tackle this problem, we propose a novel semi-supervised learning method based on multi-view learning. The main idea of our approach is generate multiple views by exploiting both feature partition and language translation strategies and then standard co-training algorithm is applied to perform multi-view learning for semi-supervised sentiment classification. Empirical study across four domains demonstrates the effectiveness of our approach. {\textcopyright} 2012 IEEE.},
author = {Su, Yan and Li, Shoushan and Ju, Shengfeng and Zhou, Guodong and Li, Xiaojun},
doi = {10.1109/IALP.2012.53},
file = {:E$\backslash$:/360yunpan/Mendeley/semi-supervised learning for imbalanced sentiment classification.pdf:pdf},
journal = {Proceedings - 2012 International Conference on Asian Language Processing, IALP 2012},
keywords = {Cross-language,Semi-supervised,Sentiment classification},
pages = {13--16},
title = {{Multi-view learning for semi-supervised sentiment classification}},
year = {2012}
}
@article{Tamposis2019,
abstract = {Motivation: Hidden Markov Models (HMMs) are probabilistic models widely used in applications in computational sequence analysis. HMMs are basically unsupervised models. However, in the most important applications, they are trained in a supervised manner. Training examples accompanied by labels corresponding to different classes are given as input and the set of parameters that maximize the joint probability of sequences and labels is estimated. A main problem with this approach is that, in the majority of the cases, labels are hard to find and thus the amount of training data is limited. On the other hand, there are plenty of unclassified (unlabeled) sequences deposited in the public databases that could potentially contribute to the training procedure. This approach is called semi-supervised learning and could be very helpful in many applications. Results: We propose here, a method for semi-supervised learning of HMMs that can incorporate labeled, unlabeled and partially labeled data in a straightforward manner. The algorithm is based on a variant of the Expectation-Maximization (EM) algorithm, where the missing labels of the unlabeled or partially labeled data are considered as the missing data. We apply the algorithm to several biological problems, namely, for the prediction of transmembrane protein topology for alpha-helical and beta-barrel membrane proteins and for the prediction of archaeal signal peptides. The results are very promising, since the algorithms presented here can significantly improve the prediction performance of even the top-scoring classifiers.},
author = {Tamposis, Ioannis A. and Tsirigos, Konstantinos D. and Theodoropoulou, Margarita C. and Kontou, Panagiota I. and Bagos, Pantelis G.},
doi = {10.1093/bioinformatics/bty910},
file = {:E$\backslash$:/360yunpan/Mendeley/Semi-supervised learning of Hidden Markov Models for biological sequence analysis.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {13},
pages = {2208--2215},
title = {{Semi-supervised learning of hidden markov models for biological sequence analysis}},
volume = {35},
year = {2019}
}
